{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Title Generation - Different Summarizations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn5bXwRutPq5"
      },
      "source": [
        "# Packages and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cODRGdyIjzwo",
        "outputId": "4c32d021-176a-499d-df3e-40ac3275f8da"
      },
      "source": [
        "!pip install sumy --quiet\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from sumy.nlp.tokenizers import Tokenizer                                       # Tokenizer\n",
        "from sumy.parsers.plaintext import PlaintextParser                              # Pasrer\n",
        "\n",
        "from sumy.summarizers.luhn import LuhnSummarizer as Luhn                        # Luhn\n",
        "from sumy.summarizers.lsa import LsaSummarizer as LSA                           # LSA Summarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer as LexRank              # Lex-Rank\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer as TextRank           # Text-Rank\n",
        "from sumy.summarizers.sum_basic import SumBasicSummarizer as SumBasic           # Sum-Basic\n",
        "from sumy.summarizers.kl import KLSummarizer as KLSum                           # KL-Sum\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 92kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 8.1MB/s \n",
            "\u001b[?25h  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV1biv0TmofW"
      },
      "source": [
        "# Summariaztion and Dataset Preparation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNqAz7K6pmET"
      },
      "source": [
        "def get_summaries(dataset, sentence_count):\n",
        "  '''\n",
        "  Input  : Dataset\n",
        "  Process: Extracts Abstracts, Generates Seven Summaries and Append\n",
        "  Returns: Combined Summaries and All Individual Summaries as Lists\n",
        "  '''\n",
        "  # Placeholders\n",
        "  combined_summaries = []\n",
        "  luhn_summaries = [] \n",
        "  lexrank_summaries = []\n",
        "  textrank_summaries = []\n",
        "  sumbasic_summaries = []\n",
        "  kl_sum_summaries = []\n",
        "  #edmundson_summaries = []\n",
        "  #ed_title_summaries = []  \n",
        "\n",
        "  abstract_list = list(dataset['Abstract'])\n",
        "  count = 1\n",
        "\n",
        "  for data in abstract_list:\n",
        "\n",
        "    # Parse and Tokenize\n",
        "    parser = PlaintextParser.from_string(data, Tokenizer(\"english\"))\n",
        "\n",
        "    # Luhn Summarizer\n",
        "    luhn_text = ''\n",
        "    luhn = Luhn()\n",
        "    luhn_summary = luhn(parser.document, sentence_count)\n",
        "    for sent in luhn_summary:\n",
        "      luhn_text += str(sent)\n",
        "    luhn_summaries.append(luhn_text)\n",
        "\n",
        "    # LexRank Summarizer\n",
        "    lex_text = ''\n",
        "    lex = LexRank()\n",
        "    lex_summary = lex(parser.document, sentence_count)\n",
        "    for sent in lex_summary:\n",
        "      lex_text += str(sent)\n",
        "    lexrank_summaries.append(lex_text)\n",
        "\n",
        "    # TextRank Summarizer\n",
        "    textrank_text = ''\n",
        "    textrank = TextRank()\n",
        "    textrank_summary = textrank(parser.document, sentence_count)\n",
        "    for sent in textrank_summary:\n",
        "      textrank_text += str(sent)\n",
        "    textrank_summaries.append(textrank_text)\n",
        "\n",
        "    # SumBasic Summarizer\n",
        "    sum_basic_text = ''\n",
        "    sumbasic = SumBasic()\n",
        "    sumbasic_summary = sumbasic(parser.document, sentence_count)\n",
        "    for sent in sumbasic_summary:\n",
        "      sum_basic_text += str(sent)\n",
        "    sumbasic_summaries.append(sum_basic_text)\n",
        "\n",
        "    # KLSum Summarizer\n",
        "    kl_text = ''\n",
        "    kl = KLSum()\n",
        "    kl_summary = kl(parser.document, sentence_count)\n",
        "    for sent in kl_summary:\n",
        "      kl_text += str(sent)\n",
        "    kl_sum_summaries.append(kl_text)\n",
        "\n",
        "    # Concatenation of Summaries\n",
        "    combined_text = luhn_text + lex_text + textrank_text + sum_basic_text + kl_text\n",
        "    combined_summaries.append(combined_text)\n",
        "\n",
        "    # Progress\n",
        "    if (count%1000 == 0):\n",
        "      print('Summarization Complete for Abstract ID = {}'.format(count))\n",
        "    count += 1\n",
        "\n",
        "  return combined_summaries, luhn_summaries, lexrank_summaries, textrank_summaries, sumbasic_summaries, kl_sum_summaries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data Preparation into Pandas Dataframe for Final Summarization Model Input and Other Tasks\n",
        "def get_data(dataset, combined_summaries, luhn_summaries, lexrank_summaries, textrank_summaries, sumbasic_summaries, kl_sum_summaries, get_excel = True):\n",
        "  '''\n",
        "  Generate Dataframe with Title and All Summaries and return final DF and an Excel File\n",
        "  '''\n",
        "  title = list(dataset['Title'])\n",
        "\n",
        "  raw_dataframe = {'Combined Abstract'   : combined_summaries, \n",
        "                   'Luhn Summaries'      : luhn_summaries,\n",
        "                   'LexRank Summaries'   : lexrank_summaries,\n",
        "                   'TextRank Summaries'  : textrank_summaries,\n",
        "                   'SumBasic Summaries'  : sumbasic_summaries,\n",
        "                   'KL Summaries'        : kl_sum_summaries,\n",
        "                   'Title'               : title}\n",
        "  df = pd.DataFrame(raw_dataframe, columns = ['Combined Abstract',\n",
        "                                              'Luhn Summaries',\n",
        "                                              'LexRank Summaries', \n",
        "                                              'TextRank Summaries',\n",
        "                                              'SumBasic Summaries',\n",
        "                                              'KL Summaries',\n",
        "                                              'Title'])\n",
        "  if (get_excel == True):\n",
        "    df.to_excel(\"Summary_Dataset_Complete.xlsx\")\n",
        "    print('Excel File Created and Saved in Local Storage.')\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol-LZQuY907r"
      },
      "source": [
        "# Execution of Summarization and Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfVJD-ANxv1z",
        "outputId": "322df0a6-2415-4e59-c465-31c78cea874c"
      },
      "source": [
        "# Parameters\n",
        "#num_examples   = 10000\n",
        "sentence_count = 2\n",
        "\n",
        "# Data Loading\n",
        "drive.mount('/content/drive')\n",
        "file = '/content/drive/MyDrive/Title Generation/Dataset/Dataset_Title_for_Summarization.xlsx' \n",
        "df = pd.read_excel(file, names = ['Abstract', 'Domain_Label', 'Title'])\n",
        "df = df.drop(['Domain_Label'], axis=1)\n",
        "#df = df[:num_examples]\n",
        "\n",
        "# Summarization\n",
        "combines, luhns, lexranks, textranks, sumbasics, kl_sums = get_summaries(df, sentence_count)\n",
        "\n",
        "# Dataset Preparation (Download Excel from Local Storage of Colab)\n",
        "final_data_frame = get_data(df, combines, luhns, lexranks, textranks, sumbasics, kl_sums, get_excel = True)\n",
        "final_data_frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Summarization Complete for Abstract ID = 1000\n",
            "Summarization Complete for Abstract ID = 2000\n",
            "Summarization Complete for Abstract ID = 3000\n",
            "Summarization Complete for Abstract ID = 4000\n",
            "Summarization Complete for Abstract ID = 5000\n",
            "Summarization Complete for Abstract ID = 6000\n",
            "Summarization Complete for Abstract ID = 7000\n",
            "Summarization Complete for Abstract ID = 8000\n",
            "Summarization Complete for Abstract ID = 9000\n",
            "Summarization Complete for Abstract ID = 10000\n",
            "Summarization Complete for Abstract ID = 11000\n",
            "Summarization Complete for Abstract ID = 12000\n",
            "Summarization Complete for Abstract ID = 13000\n",
            "Summarization Complete for Abstract ID = 14000\n",
            "Summarization Complete for Abstract ID = 15000\n",
            "Summarization Complete for Abstract ID = 16000\n",
            "Summarization Complete for Abstract ID = 17000\n",
            "Summarization Complete for Abstract ID = 18000\n",
            "Summarization Complete for Abstract ID = 19000\n",
            "Summarization Complete for Abstract ID = 20000\n",
            "Summarization Complete for Abstract ID = 21000\n",
            "Summarization Complete for Abstract ID = 22000\n",
            "Summarization Complete for Abstract ID = 23000\n",
            "Summarization Complete for Abstract ID = 24000\n",
            "Summarization Complete for Abstract ID = 25000\n",
            "Summarization Complete for Abstract ID = 26000\n",
            "Summarization Complete for Abstract ID = 27000\n",
            "Summarization Complete for Abstract ID = 28000\n",
            "Summarization Complete for Abstract ID = 29000\n",
            "Summarization Complete for Abstract ID = 30000\n",
            "Summarization Complete for Abstract ID = 31000\n",
            "Summarization Complete for Abstract ID = 32000\n",
            "Summarization Complete for Abstract ID = 33000\n",
            "Summarization Complete for Abstract ID = 34000\n",
            "Summarization Complete for Abstract ID = 35000\n",
            "Summarization Complete for Abstract ID = 36000\n",
            "Summarization Complete for Abstract ID = 37000\n",
            "Summarization Complete for Abstract ID = 38000\n",
            "Summarization Complete for Abstract ID = 39000\n",
            "Summarization Complete for Abstract ID = 40000\n",
            "Summarization Complete for Abstract ID = 41000\n",
            "Excel File Created and Saved in Local Storage.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Combined Abstract</th>\n",
              "      <th>Luhn Summaries</th>\n",
              "      <th>LexRank Summaries</th>\n",
              "      <th>TextRank Summaries</th>\n",
              "      <th>SumBasic Summaries</th>\n",
              "      <th>KL Summaries</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our single model outperforms the first place w...</td>\n",
              "      <td>Our single model outperforms the first place w...</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>The memory characteristic of the proposed recu...</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>The memory characteristic of the proposed recu...</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Que...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentences in a document or utterances in a dia...</td>\n",
              "      <td>sentences in a document or utterances in a dia...</td>\n",
              "      <td>However many short texts occur in sequences ex...</td>\n",
              "      <td>sentences in a document or utterances in a dia...</td>\n",
              "      <td>In this work we present a model based on recur...</td>\n",
              "      <td>However many short texts occur in sequences ex...</td>\n",
              "      <td>Sequential Short Text Classification with Rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>Such procedure allows training the multiresolu...</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks An ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To overcome this we introduce Sluice Networks ...</td>\n",
              "      <td>To overcome this we introduce Sluice Networks ...</td>\n",
              "      <td>Multi task learning is motivated by the observ...</td>\n",
              "      <td>We perform experiments on three task pairs and...</td>\n",
              "      <td>In Natural Language Processing NLP it is hard ...</td>\n",
              "      <td>Multi task learning is motivated by the observ...</td>\n",
              "      <td>Learning what to share between loosely relate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The system consists of an ensemble of natural ...</td>\n",
              "      <td>The system consists of an ensemble of natural ...</td>\n",
              "      <td>We present MILABOT a deep reinforcement learni...</td>\n",
              "      <td>The system consists of an ensemble of natural ...</td>\n",
              "      <td>The system consists of an ensemble of natural ...</td>\n",
              "      <td>We present MILABOT a deep reinforcement learni...</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40995</th>\n",
              "      <td>Specifically all self bounding functions can b...</td>\n",
              "      <td>Specifically all self bounding functions can b...</td>\n",
              "      <td>We study the complexity of learning and approx...</td>\n",
              "      <td>We study the complexity of learning and approx...</td>\n",
              "      <td>We study the complexity of learning and approx...</td>\n",
              "      <td>We study the complexity of learning and approx...</td>\n",
              "      <td>Nearly Tight Bounds on ell Approximation of S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40996</th>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>Even the number of users may be unknown and ca...</td>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>Concurrent bandits and cognitive radio networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40997</th>\n",
              "      <td>In particular we propose and analyze the use o...</td>\n",
              "      <td>In particular we propose and analyze the use o...</td>\n",
              "      <td>In this paper we compare and analyze clusterin...</td>\n",
              "      <td>The empirical tests and real data results show...</td>\n",
              "      <td>In this paper we compare and analyze clusterin...</td>\n",
              "      <td>In this paper we compare and analyze clusterin...</td>\n",
              "      <td>A Comparison of Clustering and Missing Data M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40998</th>\n",
              "      <td>When using CAD there is often a choice for the...</td>\n",
              "      <td>When using CAD there is often a choice for the...</td>\n",
              "      <td>Cylindrical algebraic decomposition CAD is a k...</td>\n",
              "      <td>Machine learning is the process of fitting a c...</td>\n",
              "      <td>When using CAD there is often a choice for the...</td>\n",
              "      <td>When using CAD there is often a choice for the...</td>\n",
              "      <td>Applying machine learning to the problem of c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40999</th>\n",
              "      <td>Several speaker identification systems are giv...</td>\n",
              "      <td>Several speaker identification systems are giv...</td>\n",
              "      <td>Several speaker identification systems are giv...</td>\n",
              "      <td>To deal with this problem we investigate the u...</td>\n",
              "      <td>To deal with this problem we investigate the u...</td>\n",
              "      <td>To deal with this problem we investigate the u...</td>\n",
              "      <td>A Multi Level Data Fusion Approach for Speake...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Combined Abstract  ...                                              Title\n",
              "0      Our single model outperforms the first place w...  ...   Dual Recurrent Attention Units for Visual Que...\n",
              "1      sentences in a document or utterances in a dia...  ...   Sequential Short Text Classification with Rec...\n",
              "2      We introduce the multiresolution recurrent neu...  ...   Multiresolution Recurrent Neural Networks An ...\n",
              "3      To overcome this we introduce Sluice Networks ...  ...   Learning what to share between loosely relate...\n",
              "4      The system consists of an ensemble of natural ...  ...              A Deep Reinforcement Learning Chatbot\n",
              "...                                                  ...  ...                                                ...\n",
              "40995  Specifically all self bounding functions can b...  ...   Nearly Tight Bounds on ell Approximation of S...\n",
              "40996  We consider the problem of multiple users targ...  ...    Concurrent bandits and cognitive radio networks\n",
              "40997  In particular we propose and analyze the use o...  ...   A Comparison of Clustering and Missing Data M...\n",
              "40998  When using CAD there is often a choice for the...  ...   Applying machine learning to the problem of c...\n",
              "40999  Several speaker identification systems are giv...  ...   A Multi Level Data Fusion Approach for Speake...\n",
              "\n",
              "[41000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}