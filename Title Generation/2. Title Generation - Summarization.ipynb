{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Title Generation - Summarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7tadhFvkXL4",
        "outputId": "bf1d8425-87f2-4b3b-8ede-240df6642e29"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.2MB 18.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 51.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Homa6u1dkbaZ",
        "outputId": "4105fe7c-23ed-45a9-8ce6-0052798dd6f0"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import EncoderDecoderModel, AutoTokenizer, pipeline\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"GPU Torch Available = {}\".format(torch.cuda.is_available()))\n",
        "print(\"Torch Version = {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Torch Available = True\n",
            "Torch Version = 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOBrnrVskciI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pvJKh6PkgYo"
      },
      "source": [
        "# Loading Dataset\n",
        "file = '/content/drive/MyDrive/Title Generation/Dataset/Dataset_Title_v2.xlsx' \n",
        "df = pd.read_excel(file, names = ['Abstract', 'Domain_Labels', 'Title'])\n",
        "df = df.drop(['Domain_Labels'], axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vu-BXeU8Oll"
      },
      "source": [
        "# Use this code to chop off the dataset into 5 or 6 or 7 parts and summarize\n",
        "parts_df = df[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EY8cdmbki_p"
      },
      "source": [
        "# Summarizer Initialization\n",
        "summarizer = pipeline(task = \"summarization\", model = \"t5-base\", tokenizer= \"t5-base\", framework = 'pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BgB-cL0kkip"
      },
      "source": [
        "# Data Summarization (keep max_length = 100 despite of warnings)\n",
        "train_abstracts = list(parts_df['Abstract'])\n",
        "summarized_abstracts = []\n",
        "counter = 0\n",
        "for text in train_abstracts:\n",
        "  print('Summarized {} Abstracts from Data'.format(counter+1))\n",
        "  summarized_text = summarizer(text, min_length = 20, max_length = 100, return_text = True, return_tensors = False)[0]['summary_text']\n",
        "  summarized_abstracts.append(summarized_text)\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGm_MQNbkpTY"
      },
      "source": [
        "# Data Preparation into Pandas Dataframe for Model Input\n",
        "def get_data(dataframe,sum_abs):\n",
        "  title = list(dataframe['Title'])\n",
        "  \n",
        "  raw_data_train = {'Abstract': sum_abs, 'Title': title}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['Abstract','Title'])\n",
        "  return df\n",
        "\n",
        "data = get_data(parts_df, summarized_abstracts)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL2ogosA_hGu"
      },
      "source": [
        "data.to_excel(\"Summarized_Dataset.xlsx\")  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}